#  Ollama AI Runner - Run Powerful AI Models Locally in VS Code

##  Introduction  
JAN AI is a Visual Studio Code Extension which use **DeepSeek AI** to run AI models locally, now it have support for deepseek 7B and deepseek 1.5B models, its a active development open source project and  contributions always welcome!  

##  Key Features  

###  Run AI Models Locally â€“ No Cloud Required  
Its a Github copilot alternative that runs 100% offline on your device
- Maximum Privacy & Security â€“ Your data stays on your deviceâ€”no external API calls or data leaks.  
- Faster Response Times â€“ Get instant AI-powered assistance without network latency.  
- Full Offline Functionality â€“ Run AI models anytime, even without internet access.  

###  Support for Large-Scale AI Models  
Currently, JAN AI supports:  
- Deepseek 1.5B â€“ A lightweight yet powerful AI model optimized for local execution.  
- Deepseek 7B â€“ A more advanced model offering deeper reasoning and enhanced contextual understanding.  
- Future Updates â€“ More AI models, including Mistral and LLaMA, will be supported soon!  

###  Seamless VS Code Integration  
Ollama AI Runner works effortlessly within your VS Code environment, offering:  
- Intelligent Code Suggestions â€“ Auto-complete and generate code snippets based on your prompts.  
- AI-Powered Debugging â€“ Get AI-driven insights to debug and optimize your code efficiently.  
- Context-Aware Chat â€“ Ask programming questions, get documentation explanations, and receive AI-powered guidance inside VS Code.  

###  Lightweight & Efficient  
JAN AI is optimized for performance, ensuring even large AI models run smoothly without hogging system resources.  

###  Secure & Private by Design  
- No Data Leaves Your Device â€“ Zero risk of exposing sensitive code or data to third-party services.  
- Full Control Over AI Models â€“ Customize and fine-tune models as needed without vendor restrictions.  

##  Installation & Setup  
1. Open **VS Code**  
2. Go to **Extensions** (`Ctrl + Shift + X`)  
3. Search for **"JAN AI"**  
4. Click **Install**  

ðŸ”¹ Upon installation, an **instruction site** will pop up, guiding you through:  
- Downloading and configuring the required models.  
- Setting up the local environment for smooth execution.  
- Exploring customization options for an enhanced experience.  

##  Future Roadmap  
- Support for additional AI models (e.g., Mistral, LLaMA, and more).  
- Custom model fine-tuning for personalized AI assistance. 
- Performance optimizations to handle even larger models efficiently.

##  Conclusion  
Ollama AI Runner is the perfect extension for developers who want **powerful AI capabilities** without compromising **privacy, speed, or security**. Whether youâ€™re **coding, debugging, or seeking AI-powered insights**, this extension brings the **future of local AI** directly to your fingertips.  

---  

## License
Apache 2.0 - Because sharing is caring.

ðŸ”— **[Download & Get Started Today!](https://jan-ai-extension.vercel.app/)**  

ðŸ“¢ **Need Help?** Open an issue on GitHub!  
